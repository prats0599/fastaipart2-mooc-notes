{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_11a import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're gonna use the IMDB dataset to classify positive and negative reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/jupyter/.fastai/data/imdb/ld.pkl'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/imdb.vocab'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/train'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/README'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/tmp_lm'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/ll_clas.pkl'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/test'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/unsup'),\n",
       " PosixPath('/home/jupyter/.fastai/data/imdb/tmp_clas')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a subclass of `ItemList` that will read the texts in the corresponding filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def read_file(fn):\n",
    "    with open(fn, 'r', encoding='utf8') as f: return f.read()\n",
    "    \n",
    "class TextList(ItemList):\n",
    "    @classmethod\n",
    "    def from_files(cls, path, extensions='.txt', recurse=True, include=None, **kwargs):\n",
    "        return cls(get_files(path, extensions, recurse=recurse, include=include), path, **kwargs)\n",
    "    \n",
    "    def get(self, i):\n",
    "        if isinstance(i, Path): return read_file(i)\n",
    "        return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just in case there are some log files, we restrict the ones we take to the training, test and unsupervised folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = TextList.from_files(path, include=['train', 'test', 'unsup'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should expect a total of 100k texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextList (100000 items)\n",
       "[PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/3719_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/11471_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/2165_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/8280_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/3885_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/8296_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/2521_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/10148_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/5893_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/10118_4.txt')...]\n",
       "Path: /home/jupyter/.fastai/data/imdb"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imagine pulling back the mask of a lethal assassin and finding Barbara Cartland there... that's what happens with this film.<br /><br />The opening showed promise, but soon it drops all pretenses of being a thriller (or even an imaginative love story) and the only reason they made this story becomes abundantly clear: to fill a gap in their female viewing market by creating yet another re-hash of 'mis-understood, brooding bad-boy' (Andrei) meets 'innocent, whimsical beauty' (Paula). <br /><br />Rather than waste any time in creating an original premise, the filmmakers went straight for the money-shot: the bad boy being tamed by said whimsical beauty. Thence follows a string of insincere and heavily-clichéd love scenes sprinkled with pseudo philosophical/poetic fluff. Andrei's admission of being (eponymously) a 'poet' is levered in to round out the perceived qualities a Byronic hero should have - but even when we're told in heavy, underlined writing who and what he is, it's still difficult to believe it - or care.<br /><br />For a Byronic hero/antihero to work, the story needs subtlety, style and innovation - all of which are utterly absent here. This is not a modern day Phantom of the Opera, it's just what happens when a weak and rather silly woman (with loose knicker elastic) dates a bad man, who, after meeting her, seems as dangerous as bunny slippers.<br /><br />The performances might have saved this film, had they been any good: the female lead is preoccupied with looking sexy and 'otherworldly', no matter how forced or ridiculous; and poor Dougray Scott appears to have been drugged as he shambles through his part. This is not his best work. The glimmers of interest were brought by Jürgen Prochnow as 'Vashon', and Andrew Lee Potts as the young photographer/brother. A better movie would have offed the sister and kept the brother instead.\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = tl[0]\n",
    "txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text classification, we will split the grand parent folder as before; but for language modelling, we take all the texts and just put 10% aside for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = SplitData.split_by_func(tl, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SplitData\n",
       "Train: TextList (89944 items)\n",
       "[PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/3719_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/2165_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/8280_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/3885_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/8296_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/10148_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/5893_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/10118_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/10285_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/3991_4.txt')...]\n",
       "Path: /home/jupyter/.fastai/data/imdb\n",
       "Valid: TextList (10056 items)\n",
       "[PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/11471_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/2521_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/2911_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/1338_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/9345_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/9662_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/6074_3.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/6531_4.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/3844_1.txt'), PosixPath('/home/jupyter/.fastai/data/imdb/train/neg/8821_3.txt')...]\n",
       "Path: /home/jupyter/.fastai/data/imdb"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to tokenize the dataset first, which is splitting a sentence in individual tokens. THose tokens are the basic words or punctuation signs with a few tweaks: don't for instance is split between do and n't. We will use a processor for this, in conjuction with the spacy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import spacy, html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before even tokenizing, we apply a bit of preprocessing on the texts to clean them up(we saw the one up there had some HTML code). These rules are applied before we split the sentences in tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "#special tokens\n",
    "UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ = \"xxunk xxpad xxbos xxeos xxrep xxwrep xxup xxmaj\".split()\n",
    "\n",
    "def sub_br(t):\n",
    "    \"Replaces the <br /> by \\n\"\n",
    "    re_br = re.compile(r'<\\s*br\\s*/?>', re.IGNORECASE)\n",
    "    return re_br.sub(\"\\n\", t)\n",
    "\n",
    "def spec_add_spaces(t):\n",
    "    \"Add spaces around / and #\"\n",
    "    return re.sub(r'([/#])', r' \\1 ', t)\n",
    "\n",
    "def rm_useless_spaces(t):\n",
    "    \"Removes multiple spaces\"\n",
    "    return re.sub(r' {2,}', ' ', t)\n",
    "\n",
    "def replace_rep(t):\n",
    "    \"Replaces repetitions at the character level: cccc-> TK_REP 4 c\"\n",
    "    def _replace_rep(m:Collection[str])-> str:\n",
    "        c, cc = m.groups()\n",
    "        return f' {TK_REP} {len(cc)+1} {c} '\n",
    "    re_rep = re.compile(r'(\\S)(\\1{3,})')\n",
    "    return re_rep.sub(_replace_rep, t)\n",
    "\n",
    "def replace_wrep(t):\n",
    "    \"Replace word repetitions: word word word ->> TK_WREP 3 word\"\n",
    "    def _replace_wrep(m:Collection[str])-> str:\n",
    "        c, cc = m.groups()\n",
    "        return f'{TK_WREP} {len(cc.split())+1} {c}'\n",
    "    re_wrep = re.compile(r'(\\b\\w+\\W+)(\\1{3,})')\n",
    "    return re_wrep.sub(_replace_wrep, t)\n",
    "\n",
    "def fixup_text(x):\n",
    "    \"Various messy things we've seen  in documents\"\n",
    "    re1 = re.compile(r' +')\n",
    "    x = x.replace('#39;', \"''\").replace('amp;', '&').replace('#146;', \"''\").replace(\n",
    "    'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "    '<br />', \"\\n\").replace('\\\\\"','\"').replace('<unk>', UNK).replace(' @.@ ', '.').replace(\n",
    "    ' @-@ ', '-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))\n",
    "\n",
    "default_pre_rules = [fixup_text, replace_rep, replace_wrep, spec_add_spaces, rm_useless_spaces, sub_br]\n",
    "default_spec_tok = [UNK, PAD, BOS, EOS, TK_REP, TK_WREP, TK_UP, TK_MAJ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' xxrep 4 c '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_rep('cccc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxwrep 4 word k'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_wrep('word word word word k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rules are applied AFTER the tokenization on the list of tokens(below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def replace_all_caps(x):\n",
    "    \"Replace tokens in ALL CAPS by their lower version and adds a TK_UP token before\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t.isupper() and len(t)>1: res.append(TK_UP); res.append(t.lower())\n",
    "        else: res.append(t)\n",
    "    return res\n",
    "\n",
    "def deal_caps(x):\n",
    "    \"Replace all Capitalized tokens in by their lower verison & add TK_MAJ before\"\n",
    "    res = []\n",
    "    for t in x:\n",
    "        if t == '': continue\n",
    "        if t[0].isupper() and len(t)> 1 and t[1:].islower(): res.append(TK_MAJ)\n",
    "        res.append(t.lower())\n",
    "    return res\n",
    "\n",
    "def add_bos_eos(x): return [BOS] + x + [EOS]\n",
    "\n",
    "default_post_rules = [deal_caps, replace_all_caps, add_bos_eos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxup', 'abra', 'kA', 'xxup', 'dabra']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_all_caps([\"ABRA\", \"kA\", 'DABRA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxmaj', 'my', 'name', 'is', 'xxmaj', 'jake']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deal_caps(['My', 'name', 'is', 'Jake'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since tokenizing and applying those rules takes some time, we'll parallelize it using `ProcessPoolExecutor` to go faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from spacy.symbols import ORTH\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "def parallel(func, arr, max_workers=4):\n",
    "    if max_workers<2: results = list(progress_bar(map(func, enumerate(arr)), total=len(arr)))\n",
    "    else: \n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as ex:\n",
    "            return list(progress_bar(ex.map(func, enumerate(arr)), total = len(arr)))\n",
    "    if any([o is not None for o in results]):return results     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class TokenizeProcessor(Processor):\n",
    "    def __init__(self, lang='en', chunksize=2000, pre_rules=None, post_rules=None, max_workers=4):\n",
    "        self.chunksize, self.max_workers = chunksize, max_workers\n",
    "        self.tokenizer = spacy.blank(lang).tokenizer\n",
    "#         print(default_spec_tok)\n",
    "        for w in default_spec_tok:\n",
    "            self.tokenizer.add_special_case(w, [{ORTH: w}])\n",
    "        self.pre_rules  = default_pre_rules if pre_rules is None else pre_rules\n",
    "        self.post_rules = default_post_rules if post_rules is None else post_rules\n",
    "        \n",
    "    def proc_chunk(self, args):\n",
    "        i, chunk = args\n",
    "        chunk = [compose(t, self.pre_rules) for t in chunk]\n",
    "        docs = [[d.text for d in doc] for doc in self.tokenizer.pipe(chunk)]\n",
    "        docs = [compose(t, self.post_rules) for t in docs]\n",
    "        return docs\n",
    "    \n",
    "    def __call__(self, items):\n",
    "        toks = []\n",
    "        if isinstance(items[0], Path): items = [read_file(i) for i in items]\n",
    "        chunks = [items[i:i+self.chunksize] for i in (range(0, len(items), self.chunksize))]\n",
    "        toks = parallel(self.proc_chunk, chunks, max_workers=self.max_workers)\n",
    "        return sum(toks, [])\n",
    "    \n",
    "    def proc1(self, item): return self.proc_chunk([item])[0]\n",
    "    \n",
    "    def deprocess(self, toks): return [self.deproc1(tok) for tok in toks]\n",
    "    def deproc1(self, tok): return \" \".join(tok)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp = TokenizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Imagine pulling back the mask of a lethal assassin and finding Barbara Cartland there... that's what happens with this film.<br /><br />The opening showed promise, but soon it drops all pretenses of being a thriller (or even an imaginative love story\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt[:250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"xxbos • xxmaj • imagine • pulling • back • the • mask • of • a • lethal • assassin • and • finding • xxmaj • barbara • xxmaj • cartland • there • ... • that • 's • what • happens • with • this • film • . • \\n\\n • xxmaj • the • opening • showed • promise • , • but • soon • it • drops • all • pretenses • of • being • a • thriller • ( • or • even • an • imaginative • love • story • ) • and • the • only • reason • they • made • this • story • becomes • abundantly • clear • : • to • fill • a • gap • in • their • female • viewing • market • by • creating • yet • another • re • - • hash • of • ' • mis • - • understood • , • brooding • bad • - • boy • ' • ( • xxmaj • andrei • ) • meets • ' • innocent • , • whimsical • beauty • ' • ( • xxmaj • paula • ) • . • \\n\\n • xxmaj • rather • than • waste • any • time • in • creating • an • original • premise • , • the • filmmakers • went • straight • for • the • money • - • shot • : • the • bad • boy • being • tamed • by • said • whimsical • beauty • . • xxmaj • thence • follows • a • string • of • insincere • and • heavily • - • clichéd • love • scenes • sprinkled • with • pseudo • philosophical • / • poetic • fluff • . • xxmaj • andrei • 's • admission • of • being • ( • eponymously • ) • a • ' • poet • ' • is • levered • in • to • round • out • the • perceived • qualities • a • xxmaj • byronic • hero • should • have • - • but • even • when • we • 're • told • in • heavy • , • underlined • writing • who • and • what • he • is • , • it • 's • still • difficult • to • believe • it • - • or • care • . • \\n\\n • xxmaj • for • a • xxmaj • byronic • hero • / • antihero • to • work • , • the • story • needs • subtlety • , • style • and • innovation • - • all • of • which • are • utterly • absent • here • . • xxmaj • this • is • not • a • modern • day • xxmaj • phantom • of • the • xxmaj • opera • , • it • 's • just • what • happens • when • a • weak • and • rather • silly • woman • ( • with • loose • knicker • elastic • ) • dates • a • bad • man • , • who • , • after • meeting • her • , • seems • as • dangerous • as • bunny • slippers • . • \\n\\n • xxmaj • the • performances • might • have • saved • this • film • , • had • they • been • any • good • : • the • female • lead • is • preoccupied • with • looking • sexy • and • ' • otherworldly • ' • , • no • matter • how • forced • or • ridiculous • ; • and • poor • xxmaj • dougray • xxmaj • scott • appears • to • have • been • drugged • as • he • shambles • through • his • part • . • xxmaj • this • is • not • his • best • work • . • xxmaj • the • glimmers • of • interest • were • brought • by • xxmaj • jürgen • xxmaj • prochnow • as • ' • xxmaj • vashon • ' • , • and • xxmaj • andrew • xxmaj • lee • xxmaj • potts • as • the • young • photographer • / • brother • . • a • better • movie • would • have • offed • the • sister • and • kept • the • brother • instead • . • xxeos\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' • '.join(tp(tl[:100])[0]) #[:400]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numericalizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have tokenized our texts, we replace each token by an individual number; this is called numericalizing. Again, we do this with a processor(Not so different from the `CategoryProcessor`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import collections\n",
    "\n",
    "class NumericalizeProcessor(Processor):\n",
    "    def __init__(self, vocab=None, max_vocab=60000, min_freq=2):\n",
    "        self.vocab, self.max_vocab, self.min_freq = vocab, max_vocab, min_freq\n",
    "        \n",
    "    def __call__(self, items):\n",
    "        # the vocab is defined on the first use\n",
    "        if self.vocab is None:\n",
    "            freq = Counter(p for o in items for p in o)\n",
    "            self.vocab = [o for o, c in freq.most_common(self.max_vocab) if c>=self.min_freq]\n",
    "            for o in reversed(default_spec_tok):\n",
    "                if o in self.vocab: self.vocab.remove(o)\n",
    "                self.vocab.insert(0, o) # insert the tokens in default_spec_tok\n",
    "        if getattr(self, 'otoi', None) is None:\n",
    "            self.otoi = collections.defaultdict(int, {v:k for k, v in enumerate(self.vocab)})\n",
    "        return [self.proc1(o) for o in items]\n",
    "    \n",
    "    def proc1(self, item): return [self.otoi[o] for o in item]\n",
    "    \n",
    "    def deprocess(self, idxs):\n",
    "        assert self.vocab is not None\n",
    "        return [self.deporc1(idx) for idx in idxs]\n",
    "    \n",
    "    def deproc1(self, idx): return [self.vocab[i] for i in idx]            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we do language modelling, we will infer the labels from the text during training, so there's no need to label. The training loop expects labels however, so we need to add dummy ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='45' class='' max='45', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [45/45 00:44<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='6' class='' max='6', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [6/6 00:06<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 21.2 s, sys: 2.57 s, total: 23.8 s\n",
      "Wall time: 1min 4s\n"
     ]
    }
   ],
   "source": [
    "%time ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok, proc_num])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the items have been processed they will become a list of numbers, we can still access the underlying data in `x_obj`(or `y_obj` for the targets, but we don't have them here.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"xxbos xxmaj imagine pulling back the mask of a lethal assassin and finding xxmaj barbara xxmaj cartland there ... that 's what happens with this film . \\n\\n xxmaj the opening showed promise , but soon it drops all pretenses of being a thriller ( or even an imaginative love story ) and the only reason they made this story becomes abundantly clear : to fill a gap in their female viewing market by creating yet another re - hash of ' mis - understood , brooding bad - boy ' ( xxmaj andrei ) meets ' innocent , whimsical beauty ' ( xxmaj paula ) . \\n\\n xxmaj rather than waste any time in creating an original premise , the filmmakers went straight for the money - shot : the bad boy being tamed by said whimsical beauty . xxmaj thence follows a string of insincere and heavily - clichéd love scenes sprinkled with pseudo philosophical / poetic fluff . xxmaj andrei 's admission of being ( xxunk ) a ' poet ' is xxunk in to round out the perceived qualities a xxmaj byronic hero should have - but even when we 're told in heavy , underlined writing who and what he is , it 's still difficult to believe it - or care . \\n\\n xxmaj for a xxmaj byronic hero / antihero to work , the story needs subtlety , style and innovation - all of which are utterly absent here . xxmaj this is not a modern day xxmaj phantom of the xxmaj opera , it 's just what happens when a weak and rather silly woman ( with loose knicker elastic ) dates a bad man , who , after meeting her , seems as dangerous as bunny slippers . \\n\\n xxmaj the performances might have saved this film , had they been any good : the female lead is preoccupied with looking sexy and ' otherworldly ' , no matter how forced or ridiculous ; and poor xxmaj dougray xxmaj scott appears to have been drugged as he shambles through his part . xxmaj this is not his best work . xxmaj the glimmers of interest were brought by xxmaj jürgen xxmaj prochnow as ' xxmaj xxunk ' , and xxmaj andrew xxmaj lee xxmaj potts as the young photographer / brother . a better movie would have offed the sister and kept the brother instead . xxeos\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll.train.x_obj(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the preprocessing step takes time, we save the intermediate result using pickle.   \n",
    "NOTE: Don't use any lambda functions in your processors or they won't be able to pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ld.pkl', 'wb'))\n",
    "pickle.dump(proc_num, open(path/'proc_num_vocab.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ld.pkl', 'rb'))\n",
    "proc_num.vocab = pickle.load(open(path/'proc_num_vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a bit of work to convert our `LabelList` in a `DataBunch` as we don't just want batches of IMDB reviews. We want to stream through all the texts concatenated. We also have to prepare the targets that are the next words in the text. All of this is done with the next object called `LM_Dataset`. At the beginning of the each epoch, it'll shuffle the articles (if `shuffle=True`) and create a big stream in `bs` smaller streams. That will read in chunks of bptt length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say our stream is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='1' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [1/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream = \"\"\"\n",
    "In this notebook, we will go back over the example of classifying movie reviews we studied in part 1 and dig deeper under the surface. \n",
    "First we will look at the processing steps necessary to convert text into numbers and how to customize it. By doing this, we'll have another example of the Processor used in the data block API.\n",
    "Then we will study how we build a language model and train it.\\n\n",
    "\"\"\"\n",
    "tokens = np.array(tp([stream])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we split it in 6 batches it would give something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs ,seq_len = 6, 15\n",
    "d_tokens = np.array([tokens[i*seq_len:(i+1)*seq_len] for i in range(bs)])\n",
    "df = pd.DataFrame(d_tokens)\n",
    "display(HTML(df.to_html(index=False, header=None)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then if we have a `bptt` of 5, we would go over those 3 batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>xxbos</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>in</td>\n",
       "      <td>this</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>classifying</td>\n",
       "      <td>movie</td>\n",
       "      <td>reviews</td>\n",
       "      <td>we</td>\n",
       "      <td>studied</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>first</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>numbers</td>\n",
       "      <td>and</td>\n",
       "      <td>how</td>\n",
       "      <td>to</td>\n",
       "      <td>customize</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>another</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "      <td>the</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>then</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>study</td>\n",
       "      <td>how</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>notebook</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>will</td>\n",
       "      <td>go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>in</td>\n",
       "      <td>part</td>\n",
       "      <td>1</td>\n",
       "      <td>and</td>\n",
       "      <td>dig</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>look</td>\n",
       "      <td>at</td>\n",
       "      <td>the</td>\n",
       "      <td>processing</td>\n",
       "      <td>steps</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>xxmaj</td>\n",
       "      <td>by</td>\n",
       "      <td>doing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>processor</td>\n",
       "      <td>used</td>\n",
       "      <td>in</td>\n",
       "      <td>the</td>\n",
       "      <td>data</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>we</td>\n",
       "      <td>build</td>\n",
       "      <td>a</td>\n",
       "      <td>language</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>back</td>\n",
       "      <td>over</td>\n",
       "      <td>the</td>\n",
       "      <td>example</td>\n",
       "      <td>of</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>deeper</td>\n",
       "      <td>under</td>\n",
       "      <td>the</td>\n",
       "      <td>surface</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>necessary</td>\n",
       "      <td>to</td>\n",
       "      <td>convert</td>\n",
       "      <td>text</td>\n",
       "      <td>into</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>this</td>\n",
       "      <td>,</td>\n",
       "      <td>we</td>\n",
       "      <td>'ll</td>\n",
       "      <td>have</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>block</td>\n",
       "      <td>api</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n</td>\n",
       "      <td>xxmaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>and</td>\n",
       "      <td>train</td>\n",
       "      <td>it</td>\n",
       "      <td>.</td>\n",
       "      <td>\\n\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bs, bptt = 6,5\n",
    "for k in range(3):\n",
    "    d_tokens = np.array([tokens[i*seq_len + k*bptt: i*seq_len + (k+1) * bptt] for i in range(bs)])\n",
    "    df = pd.DataFrame(d_tokens)\n",
    "    display(HTML(df.to_html(index=False, header=None)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LM_Dataset():\n",
    "    def __init__(self, data, bs=64, bptt=72, shuffle=False):\n",
    "        self.data, self.bs, self.bptt, self.shuffle = data, bs, bptt, shuffle\n",
    "        total_len = sum([len(t) for t in data.x])\n",
    "        self.n_batch = total_len // bs\n",
    "        self.batchify()\n",
    "        \n",
    "    def __len__(self): return ((self.n_batch-1) // self.bptt) * self.bs\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        source = self.batched_data[idx % self.bs]\n",
    "        seq_idx = (idx // self.bs) * self.bptt\n",
    "        return source[seq_idx: seq_idx+self.bptt], source[seq_idx+1: seq_idx+self.bptt+1] # x, y\n",
    "    \n",
    "    def batchify(self):\n",
    "        texts = self.data.x\n",
    "        if self.shuffle: texts = texts[torch.randperm(len(texts))]\n",
    "        stream = torch.cat([tensor(t) for t in texts])\n",
    "        self.batched_data = stream[:self.n_batch * self.bs].view(self.bs, self.n_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = DataLoader(LM_Dataset(ll.valid, shuffle=True), batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if it all works ok: `x1`, `y1`, `x2`, and `y2` should all be of size `bs` by `bptt`. The texts in each row of `x1` should continue in `x2`. `y1` and `y2` should have the same texts as their x counterpart shifted by 1 position to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(dl)\n",
    "x1, y1 = next(iter_dl)\n",
    "x2, y2 = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 72]), torch.Size([64, 72]))"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1.size(), y1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = proc_num.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxbos xxmaj in many ways , the filmic career of independent film - making legend xxmaj john xxmaj cassavetes is the polar opposite of someone like xxmaj alfred xxmaj hitchcock , the consummate studio director . xxmaj where xxmaj hitchcock infamously treated his actors as cattle , xxmaj cassavetes sought to work with them xxunk . xxmaj where every element in a xxmaj hitchcock shot is composed immaculately , xxmaj cassavetes cared'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'xxmaj in many ways , the filmic career of independent film - making legend xxmaj john xxmaj cassavetes is the polar opposite of someone like xxmaj alfred xxmaj hitchcock , the consummate studio director . xxmaj where xxmaj hitchcock infamously treated his actors as cattle , xxmaj cassavetes sought to work with them xxunk . xxmaj where every element in a xxmaj hitchcock shot is composed immaculately , xxmaj cassavetes cared less'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in y1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"less for the way a scene was figuratively composed than in how it felt , or what it conveyed , emotionally . xxmaj hitchcock 's tales were always plot - first narratives , with the human element put in the background . xxmaj cassavetes put the human experience forefront in every one of his films . xxmaj if some things did not make much sense logically , so be it . \\n\\n\""
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(vocab[o] for o in x2[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare a convenience function to do this quickly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we don't compute gradients for valid_ds, we usually set it's bs twice that to train_ds "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_lm_dls(train_ds, valid_ds, bs, bptt, **kwargs):\n",
    "    return (DataLoader(LM_Dataset(train_ds, bs, bptt, shuffle=True), batch_size=bs, **kwargs),\n",
    "           DataLoader(LM_Dataset(valid_ds, bs, bptt, shuffle=False), batch_size=2*bs, **kwargs))\n",
    "\n",
    "def lm_databunchify(sd, bs, bptt, **kwargs):\n",
    "    return DataBunch(*get_lm_dls(sd.train, sd.valid, bs, bptt, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs ,bppt = 64, 72\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batching for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we will want to tackle classification, gathering the data will be a bit different: first we will label our texts with the folder they come from, and then we will need to apply padding to batch them together. To avoid mixing very long texts with very short ones, we will also use `Sampler` to sort(with a bit of randomness for the training set) ur samples by length.  \n",
    "  \n",
    "First, the datablock API calls should look familiar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_cat = CategoryProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='13' class='' max='13', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      100.00% [13/13 00:13<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tl = TextList.from_files(path, include=['train', 'test'])\n",
    "sd = SplitData.split_by_func(tl, partial(grandparent_splitter, valid_name='test'))\n",
    "ll = label_by_func(sd, parent_labeler, proc_x = [proc_tok, proc_num], proc_y = proc_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ll, open(path/'ll_clas.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_clas.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if the labels are consistent with the texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('xxbos xxmaj is it a coincidence that xxmaj orca was made two years after xxmaj jaws ? xxmaj orca is n\\'t exactly a \" xxmaj jaws rip off \" but it is obvious that it tried to profit from xxmaj jaws \\'s success . xxmaj first of all xxmaj orca in my opinion was a bad movie , not terrible but definitely not good , average at best . \\n\\n xxmaj the plot is basically a male killer whale ( orca ) after seeing its mate and its unborn calf killed by a fisherman seeks revenge . i could n\\'t stand to watch this movie again . xxmaj the direction of this film is poor and when compared to xxmaj jaws it looks like the director , producers , and writers were almost talentless . \\n\\n xxmaj as for the acting , it was very average and believable , however the actual characters are n\\'t the least bit likable . xxmaj the effects were alright for its time and the footage of the killer whale looked pretty good . \\n\\n xxmaj the violence is confusing , bloody , and not recommended for more sensitive people . xxmaj the music is overdone and very loud , drowning out the sound effects and irritating at times . i hated the way they exaggerated the intelligence of the killer whale ( killer whales do n\\'t mate with only one mate as depicted in xxmaj orca ) . \\n\\n xxmaj overall this movie was bad / poor in my opinion , because of the reasons listed above . xxmaj some people may appreciate this film more because of the concept of vengeance amongst animals and humans so i \\'m not going to bash this movie and i can understand why some people may like it . \\n\\n xxmaj my xxmaj rating : 3.5 / 10 ( but for its concept possibly a 5 / 10 ) xxeos',\n",
       "  'neg'),\n",
       " ('xxbos xxmaj listening to the director \\'s commentary confirmed what i had suspected whilst watching the film : this is a movie made by a guy who wants to play at making a movie . xxmaj the plot is the kind of thing that deluded teenagers churn out when they \\'re going through that \" i could write a book / screenplay / award winning sitcom \" phase . xxmaj there \\'s a germ of an interesting idea buried in there ( probably because its a sequel to some - one else \\'s movie ) , but it is totally buried under an underwritten , badly executed and laughably un - thought - out script . \\n\\n xxmaj the lines are dire , and the performances are un - engaging , though again , i \\'m inclined to blame the director . xxmaj he does not appear to have consulted the actors at all about what is required , rather plonked the script in their hands , pointed the camera at them and told them to get on with it . xxmaj who knows , with a little coaching , these actors could have acquitted themselves better ( say what you like about musicians in movies , xxmaj jon xxmaj bon xxmaj jovi was excellent in xxmaj row xxmaj your xxmaj boat and more than acceptable in xxmaj the xxmaj leading xxmaj man ) . \\n\\n xxmaj as it stands , the cast have no chemistry whatsoever . a beautiful opportunity to use the classic sex and vampirism parallel is passed up when , in order to infect xxmaj bon xxmaj jovi \\'s character with vampire blood from his ailing co - hunter , he is given a transfusion . xxmaj she should have bitten him . xxmaj mind you , they should have looked vaguely interested in each other throughout the rest of the film too . xxmaj the only real moment of sexual tension , between the two female leads , is by the directors own admittance accidental . xxmaj he had originally intended to use this silent sequence as an excuse for more pointless plot exposition - so , i suppose the finished product could have conceivably been worse . xxmaj but not a lot . \\n\\n xxmaj frankly , as movies go , this is badly plotted , silly and forgettable . xxmaj even as trashy movies go it \\'s not sexy enough or gory enough to be entertaining . xxmaj it could have been a fun and bloody little romp , but the director has left with more of a comedy , for all the wrong reasons . xxeos',\n",
       "  'neg')]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(ll.train.x_obj(i), ll.train.y_obj(i)) for i in [1, 12000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['xxbos xxmaj is it a coincidence that xxmaj orca was made two years after xxmaj jaws ? xxmaj orca is n\\'t exactly a \" xxmaj jaws rip off \" but it is obvious that it tried to profit from xxmaj jaws \\'s success . xxmaj first of all xxmaj orca in my opinion was a bad movie , not terrible but definitely not good , average at best . \\n\\n xxmaj the plot is basically a male killer whale ( orca ) after seeing its mate and its unborn calf killed by a fisherman seeks revenge . i could n\\'t stand to watch this movie again . xxmaj the direction of this film is poor and when compared to xxmaj jaws it looks like the director , producers , and writers were almost talentless . \\n\\n xxmaj as for the acting , it was very average and believable , however the actual characters are n\\'t the least bit likable . xxmaj the effects were alright for its time and the footage of the killer whale looked pretty good . \\n\\n xxmaj the violence is confusing , bloody , and not recommended for more sensitive people . xxmaj the music is overdone and very loud , drowning out the sound effects and irritating at times . i hated the way they exaggerated the intelligence of the killer whale ( killer whales do n\\'t mate with only one mate as depicted in xxmaj orca ) . \\n\\n xxmaj overall this movie was bad / poor in my opinion , because of the reasons listed above . xxmaj some people may appreciate this film more because of the concept of vengeance amongst animals and humans so i \\'m not going to bash this movie and i can understand why some people may like it . \\n\\n xxmaj my xxmaj rating : 3.5 / 10 ( but for its concept possibly a 5 / 10 ) xxeos',\n",
       " \"xxbos xxmaj although , i had no earthly idea on what to expect from this movie , this sure as hell was n't what i would have had in mind , had anything actually come to mind . xxmaj once i heard of its existence , all i knew was that i had to own a movie called xxmaj please xxmaj do n't xxmaj eat xxmaj the xxmaj babies . unfortunately , i could only find a copy under its alternate title , xxmaj island xxmaj fury . xxmaj looking back , i guess i could call it a lose - lose situation . xxmaj on one hand , i still do n't get to be known as the guy who owns a movie called xxmaj please xxmaj do n't xxmaj eat xxmaj the xxmaj babies , and on the other hand , xxmaj island xxmaj fury would ultimately reveal itself to be an awful , pointless , boring , unwatchable piece of garbage . xxmaj yeah , definitely lose - lose . \\n\\n i 'm not even sure what genre they 're going for here . xxmaj just early 80 's badness , with a flashback that might actually be longer than the non - flashback . xxmaj first up , two teenage girls are being chased by two bad guys , once caught , the bad guys bring to our attention that one of the girls have a coin on a string , around her neck , and somehow , these bad guys know of a lot more of these coins hidden on an island somewhere . xxmaj and this is where things start to get weird , somehow these guys know of a trip the girls took to some island , years earlier , when they were only 10 . i guess this is supposed to mean that the girls should know exactly where this alleged treasure is . xxmaj so , now , we 're in the past , while the girls try to retrace their steps , so these bad guys do n't kill them , although , i would n't have minded if they had . xxmaj in the flashback , the 10 year old counterparts are on a boat trip with their sisters and the sisters boyfriends , eventually stopping by an island for some air , they get mixed up with some kid and his killer grandparents . xxmaj any potential suspense or reasons to keep on watching never shows up , but the flashback was undeniably better than the present , which , still , is n't saying much . \\n\\n xxmaj for a while there i had forgotten about the original story , xxmaj at one point , i xxmaj xxunk maybe the director had too , and when the flashback ended , that would be the end , which would have worked for me considering this disappointment would have been a half - hour shorter . xxmaj this pointless movie within a pointless movie does eventually end , and real stuff does happen , but it 's stupid . i guess i did n't exactly expect a movie filled with infants being devoured , or anything like that , but i did expect some form of outlandish b - entertainment , mostly just a confusing , inept storyline , unsure of its genre . xxmaj my advice would be to seek out something worthwhile like xxmaj attack xxmaj of xxmaj the xxmaj beast xxmaj creatures . xxmaj if anyone , i would only recommend this one to serious b - movie collectors who must have them all , anyone else interested probably has brain damage . xxmaj what really gets me is that i still have no idea why they called it xxmaj please xxmaj do n't xxmaj eat xxmaj the xxmaj babies . 3 / 10 xxeos\"]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[ll.train.x_obj(i) for i in [1, 10]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw samplers in notebook 3. For the valid_ds, we will simply sort the samples by length and we will begin w the longest ones for memory reasons(it's better to always have the biggest tensors first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class SortSampler(Sampler):\n",
    "    def __init__(self, data_source, key): self.data_source, self.key = data_source, key\n",
    "    def __len__(self): return len(self.data_source)\n",
    "    def __iter__(self): return iter(sorted(list(range((len(self.data_source)))), key=self.key, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampler??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the training set, we want some kind of randomness on top of this. So first, we shuffle the texts and build megabatches of size `50*bs`. We sort those megabatches by length before splitting them in 50 mini-batches. That way we'll have randomized batches of roughly the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we make sure to have the biggest batch first and shuffle the order of other batches. We also make sure that the last batch stays at the end because it's size is probably smaller than the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SortishSampler(Sampler):\n",
    "    def __init__(self, data_source, key, bs):\n",
    "            self.data_source, self.key, self.bs = data_source, key, bs\n",
    "        \n",
    "    def __len__(self)->int: return len(self.data_source)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        idxs = torch.randperm(len(self.data_source))\n",
    "        megabatches = [idxs[i:i+self.bs*50] for i in range(0, len(idxs), self.bs*50)]\n",
    "        sorted_idx = torch.cat([tensor(sorted(s, key=self.key, reverse=True)) for s in megabatches])\n",
    "        batches = [sorted_idx[i:i+self.bs] for i in range(0, len(sorted_idx), self.bs)]\n",
    "        max_idx = torch.argmax(tensor([self.key(ck[0]) for ck in batches])) # find the chunk with the largest key\n",
    "        batches[0], batches[max_idx] = batches[max_idx], batches[0] # then make sure it goes first\n",
    "        # make sure the largest and smallest batch goes first and last and randomize the order of the rest.\n",
    "        batch_idxs = torch.randperm(len(batches)-2)\n",
    "        sorted_idx = torch.cat([batches[i+1] for i in batch_idxs]) if len(batches)>1 else LongTensor([])\n",
    "        sorted_idx = torch.cat([batches[0], sorted_idx, batches[-1]])\n",
    "        return iter(sorted_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Padding: we had the padding token(as an id of 1) at the end of each sequence to make them all the same size when batching them. Note that we need padding at the end to be able to use `Pytorch` convenience functions that will let us ignore that padding(see 12c)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def pad_collate(samples, pad_idx=1, pad_first=False):\n",
    "    max_len = max([len(s[0]) for s in samples])\n",
    "    res = torch.zeros(len(samples), max_len).long() + pad_idx\n",
    "    for i,s in enumerate(samples):\n",
    "        if pad_first: res[i, -len(s[0]):] = LongTensor(s[0])\n",
    "        else:         res[i, :len(s[0]) ] = LongTensor(s[0])\n",
    "    return res, tensor([s[1] for s in samples])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 64\n",
    "train_sampler = SortishSampler(ll.train.x, key=lambda t: len(ll.train[int(t)][0]), bs=bs)\n",
    "train_dl = DataLoader(ll.train, batch_size=bs, sampler=train_sampler, collate_fn = pad_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "iter_dl = iter(train_dl)\n",
    "x, y = next(iter_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 3311]), torch.Size([64]))"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3311, 1699, 1394, 1390, 1355], 1049)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1)- (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last one is minimal length. This is the first batch so it has the longest sequence, but if we look at the next one that is more random, we see lengths are roughly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([449, 448, 448, 447, 447], 424)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x, y = next(iter_dl)\n",
    "lengths = []\n",
    "for i in range(x.size(0)): lengths.append(x.size(1)- (x[i]==1).sum().item())\n",
    "lengths[:5], lengths[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the padding in the end(id is 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   2,   18,   73,  ...,   89,   66,    3],\n",
       "        [   2,    7, 1066,  ...,    9,    3,    1],\n",
       "        [   2,    7,   19,  ...,    9,    3,    1],\n",
       "        ...,\n",
       "        [   2,    7,   19,  ...,    1,    1,    1],\n",
       "        [   2,   18,  319,  ...,    1,    1,    1],\n",
       "        [   2,   18,  235,  ...,    1,    1,    1]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we add a convenience function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_class_dls(train_ds, valid_ds, bs, **kwargs):\n",
    "    train_sampler = SortishSampler(train_ds.x, key=lambda t: len(train_ds.x[t]), bs=bs)\n",
    "    valid_sampler = SortSampler(valid_ds.x, key=lambda t: len(valid_ds.x[t]))\n",
    "    return (DataLoader(train_ds, batch_size=bs, sampler=train_sampler, collate_fn=pad_collate, **kwargs),\n",
    "           DataLoader(valid_ds, batch_size=bs*2, sampler=valid_sampler, collate_fn=pad_collate, **kwargs))\n",
    "\n",
    "def clas_databunchify(sd, bs, **kwargs):\n",
    "    return DataBunch(*get_class_dls(sd.train, sd.valid, bs, **kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs, bptt = 64, 72\n",
    "data = clas_databunchify(ll, bs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12_text.ipynb to exp/nb_12.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 12_text.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
