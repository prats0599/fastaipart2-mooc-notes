{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AWD_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from exp.nb_12 import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = datasets.untar_data(datasets.URLs.IMDB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to preprocess the data again to pickle it because if we try to load the previous `SplitLabeledData` with pickle, it will complain that some functions aren't in main."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "il = TextList.from_files(path, include=['train', 'test', 'unsup'])\n",
    "sd = SplitData.split_by_func(il, partial(random_splitter, p_valid=0.1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_tok, proc_num = TokenizeProcessor(max_workers=8), NumericalizeProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ll = label_by_func(sd, lambda x: 0, proc_x = [proc_tok, proc_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle.dump(ll, open(path/'ll_lm.pkl', 'wb'))\n",
    "# pickle.dump(proc_num.vocab, open(path/'vocab_lm.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = pickle.load(open(path/'ll_lm.pkl', 'rb'))\n",
    "vocab = pickle.load(open(path/'vocab_lm.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs,bptt = 64,70\n",
    "data = lm_databunchify(ll, bs, bptt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWD-LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before studying about awd-lstm, let's look at what an LSTM is. NNs were covered in part 1, if you need a refresher, there is a great visualization of them on [this website](http://joshvarty.github.io/VisualizingRNNs/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to implement those equations(where σ stands for sigmoid):\n",
    "![LSTM cell and equations](images/lstm.jpg)\n",
    "(picture from [Understanding LSTMs](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) by Chris Olah.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to take advantage of our GPU, it's better to do one big matrix multiplication than 4 smaller ones SO we compute the values of four gates all at once. SInce there is matrix multiplication and a bias, we use `nn.Linear` to do it.  \n",
    "  \n",
    "We need 2 lineaar layers: one for the input and one of the hidden state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        self.ih = nn.Linear(ni, 4*nh) # times 4 because you divide it in 4 chunks later\n",
    "        self.hh = nn.Linear(nh, 4*nh)\n",
    "    \n",
    "    def forward(self, input, state):\n",
    "        h, c = state\n",
    "        # one big multiplication for all the gates is better than 4 smaller ones\n",
    "        gates = (self.ih(input) + self.hh(h)).chunk(4, 1) # split them into 4 chunks along the 1st dimension\n",
    "        ingate, forgetgate, outgate = map(torch.sigmoid, gates[:3])\n",
    "        cellgate = gates[3].tanh()\n",
    "        \n",
    "        c = (forgetgate*c) + (ingate*cellgate)\n",
    "        h = outgate * c.tanh()\n",
    "        return h, (h, c) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the LSTM layer just applies the cell on all the timesteps in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMLayer(nn.Module):\n",
    "    def __init__(self, cell, *cell_args):\n",
    "        super().__init__()\n",
    "        self.cell = cell(*cell_args)\n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        inputs = input.unbind(1) # removes tensor dimension(1)\n",
    "        #>>> torch.unbind(torch.tensor([[1, 2, 3],\n",
    "        # >>>                            [4, 5, 6],\n",
    "        # >>>                            [7, 8, 9]]))\n",
    "        # (tensor([1, 2, 3]), tensor([4, 5, 6]), tensor([7, 8, 9]))\n",
    "        outputs = []\n",
    "        for i in range(len(inputs)):\n",
    "            out, state = self.cell(inputs[i], state)\n",
    "            outputs += [out]\n",
    "        return torch.stack(outputs, dim=1), state        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try it out and see how fast we are. We ONLY measure the forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTMLayer(LSTMCell, 1000, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64, 70, 1000)\n",
    "h = (torch.randn(64,1000), torch.zeros(64 ,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "713 ms ± 8.53 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 y,h1 = lstm(x,h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synchronize is used to make sure that things \n",
    "# execute in the cuda as well as the python world.\n",
    "def time_fn(f):\n",
    "    f()\n",
    "    torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm ,x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.2 ms ± 389 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built in version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare with Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM(1000, 1000, batch_first=True)\n",
    "# Without batch_first=True it will use the first dimension as the sequence dimension.\n",
    "# With batch_first=True it will use the second dimension as the sequence dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(64,70, 1000)\n",
    "h = (torch.zeros(1,64,1000), torch.zeros(1, 64, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610 ms ± 3.7 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10  y, h1 = lstm(x, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = lstm.cuda()\n",
    "x = x.cuda()\n",
    "h = (h[0].cuda(), h[1].cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = partial(lstm, x, h)\n",
    "time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.2 ms ± 439 µs per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 time_fn(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SO our version is running at almost the same speed on the CPU. However on the GPU, Pytorhc uses CUDNN behind the scenes that optimizes greatly the for loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jit version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "\n",
    "We want to use the AWD-LSTM from [Stephen Merity et al.](https://arxiv.org/abs/1708.02182). First we'll need all different kinds of dropouts. Dropout consists into replacing some coefficients by 0 with probability p. To ensure that the average of the weights remains constant, we apply a correction to the weights that aren't nullified of a fastor `1/(1-p)` (think of what happens to the activations if you want to figure out why--> Ans: they'll either explode or vanish).  \n",
    "  \n",
    "We apply dropout by drawing a mask that tells us which elemnts to nullify or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def dropout_mask(x, sz, p):\n",
    "    return x.new(*sz).bernoulli_(1-p).div(1-p)\n",
    "# bernoulli simply means create 1s nad 0s and it's 1 with the probability of (1-p) and divide each element by (1-p)\n",
    "# SO if p=0.5 you get a matrix of 0s and 2s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 2., 0., 2., 2., 0., 0.],\n",
       "        [2., 2., 0., 0., 0., 2., 2., 2., 2., 2.],\n",
       "        [2., 2., 2., 2., 2., 2., 2., 2., 2., 2.],\n",
       "        [0., 2., 2., 2., 0., 2., 0., 2., 0., 0.],\n",
       "        [2., 0., 2., 2., 2., 0., 0., 0., 2., 0.],\n",
       "        [0., 0., 2., 0., 0., 2., 0., 2., 2., 2.],\n",
       "        [0., 2., 2., 2., 2., 2., 2., 0., 2., 2.],\n",
       "        [2., 0., 2., 0., 2., 2., 0., 0., 0., 2.],\n",
       "        [2., 0., 0., 0., 0., 0., 2., 0., 2., 0.],\n",
       "        [2., 0., 0., 2., 2., 0., 2., 2., 0., 2.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(10, 10)\n",
    "mask = dropout_mask(x, (10, 10), 0.5); mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once with a dropout mask, applying the dropout to `x` is simply done by `x = x*mask`. We create our own dropout mask and don't rely on pytorch dropout because we don't want to nullify the coefficients randomly: on the sequence dimension, we will want to have always replace the same positions by zero along the seq_len dimension(WHY??)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.2104), tensor(0.8589))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x*mask).std(), x.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside a RNN, a tensor x will have 3 dimensions: bs, seq_len, vocab_size. Recall that we want to consistently apply the dropout mask across the seq_len dimension, therfore: we create a dropout mask for the first and third dimension and broadcast it to the seq_len dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNDropout(nn.Module):\n",
    "    def __init__(self, p=0.5):\n",
    "        super().__init__()\n",
    "        self.p = p\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if not self.training or self.p == 0. : return x\n",
    "        m = dropout_mask(x.data, (x.size(0), 1, x.size(2)), self.p)\n",
    "        return x * m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1570, -0.0000, -0.7048,  0.0000, -0.2125, -0.0000, -1.2395],\n",
       "         [ 1.6299,  0.0000, -0.0185,  0.0000,  1.6618, -0.0000,  1.7363],\n",
       "         [-0.2886,  0.0000,  1.3313, -0.0000,  2.2231,  0.0000,  0.7224]],\n",
       "\n",
       "        [[-0.0000,  3.3907, -1.1589, -0.4866,  0.6657, -1.7509, -1.3297],\n",
       "         [ 0.0000,  1.0690, -0.3558,  0.3287,  0.8011, -1.5387,  1.9071],\n",
       "         [ 0.0000, -1.3100,  0.9358,  0.0100, -1.3165,  0.8615, -0.2637]],\n",
       "\n",
       "        [[-0.0000,  0.0000, -2.3151,  1.1073, -0.0000, -2.5797,  0.0000],\n",
       "         [ 0.0000, -0.0000,  0.1859,  0.6219, -0.0000, -1.1936, -0.0000],\n",
       "         [-0.0000,  0.0000, -1.4853, -2.2439, -0.0000, -1.4298,  0.0000]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp = RNNDropout(0.3)\n",
    "tst_input = torch.randn(3,3,7)\n",
    "# tst_input\n",
    "dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the zeros are getting broadcasted. This is  really important bacause in the seq dimension if you drop timestep 3 butnot timestep 2 and timestep4 then you'vebroken the network's ability to calculate anything because you just killed it.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WeightDrop (called as DropConnect in Vision world)\n",
    "\n",
    "WeightDropout is the dropout applied to the weights of the inner LSTM hidden to hidden matrix. This is a little hacky if we want to preserve the CUDNN speed and not reimplement the ell from scratch. We add a parameter that will contain the raw weights, and we replace the weight matrix in the LSTM at the beginning of the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import warnings\n",
    "\n",
    "WEIGHT_HH = 'weight_hh_l0' # small L\n",
    "\n",
    "class WeightDropout(nn.Module):\n",
    "    def __init__(self, module, weight_p=[0.], layer_names=[WEIGHT_HH]):\n",
    "        super().__init__()\n",
    "        self.module, self.weight_p, self.layer_names = module, weight_p, layer_names\n",
    "        for layer in layer_names:\n",
    "            # Makes a copy of the weights of selected layers.\n",
    "            w= getattr(self.module, layer)\n",
    "            self.register_parameter(f'{layer}_raw', nn.Parameter(w.data))\n",
    "            self.module._parameters[layer] = F.dropout(w, p=self.weight_p, training = False)\n",
    "     \n",
    "    def _setweights(self):\n",
    "        # applying dropout on the weights\n",
    "        for layer in self.layer_names:\n",
    "            raw_w = getattr(self, f'{layer}_raw')\n",
    "            self.module._parameters[layer] = F.dropout(raw_w, p = self.weight_p, training = self.training)\n",
    "        \n",
    "    def forward(self, *args):\n",
    "        self._setweights()\n",
    "        with warnings.catch_warnings():\n",
    "            # To avoid the warning that comes because the weights aren't flattended\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            return self.module.forward(*args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.2388, -0.0573],\n",
       "        [-0.1495,  0.6926],\n",
       "        [ 0.3976,  0.6529],\n",
       "        [ 0.2691,  0.4294],\n",
       "        [-0.6123,  0.0434],\n",
       "        [ 0.1087, -0.3028],\n",
       "        [ 0.0889, -0.1240],\n",
       "        [-0.5016,  0.0384]], requires_grad=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module = nn.LSTM(5,2)\n",
    "dp_module = WeightDropout(module, 0.4)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, (tensor([[[0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.]]]), tensor([[[0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.],\n",
       "           [0., 0.]]])))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = (torch.zeros(1, 20, 2), torch.zeros(1, 20, 2))\n",
    "len(h), h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's at the beginning of the fwd pass that the dropout is applied to the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3981, -0.0000],\n",
       "        [-0.2491,  1.1544],\n",
       "        [ 0.6627,  0.0000],\n",
       "        [ 0.4485,  0.0000],\n",
       "        [-1.0206,  0.0723],\n",
       "        [ 0.1811, -0.5047],\n",
       "        [ 0.1482, -0.0000],\n",
       "        [-0.0000,  0.0640]], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tst_input = torch.randn(4, 20, 5)\n",
    "h = (torch.zeros(1, 20, 2), torch.zeros(1, 20, 2))\n",
    "x, h = dp_module(tst_input, h)\n",
    "getattr(dp_module.module, WEIGHT_HH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Dropout\n",
    "Embedding Dropout applies dropout to full rows of the embedding matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "class EmbeddingDropout(nn.Module):\n",
    "    \" Applies Dropout in the embedding layer by zeroing out some elements of the embedding vector.\" \n",
    "    def __init__(self, emb, embed_p):\n",
    "        super().__init__()\n",
    "        self.emb , self.embed_p = emb, embed_p\n",
    "        self.pad_idx = self.emb.padding_idx\n",
    "        if self.pad_idx is None: self.pad_idx = -1\n",
    "            \n",
    "    def forward(self, words, scale=None):\n",
    "        if self.training and self.embed_p!=0:\n",
    "            size = (self.emb.weight.size(0), 1)\n",
    "            mask = dropout_mask(self.emb.weight.data, size, self.embed_p)\n",
    "            masked_embed = self.emb.weight * mask\n",
    "        else:masked_embed = self.emb_weight\n",
    "        if scale: masked_embed.mul_(scale)\n",
    "        return F.embedding(words, masked_embed, self.pad_idx, self.emb.max_norm,\n",
    "                           self.emb.norm_type, self.emb.scale_grad_by_freq, self.emb.sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([97, 64, 65, 49,  6, 54, 21, 61])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-4.0852,  1.2072,  2.5415,  4.6441,  4.7406,  3.0731, -2.6044],\n",
       "        [ 0.0000,  0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 1.7719, -3.9148, -2.8937, -5.5264, -0.1590, -1.8944,  3.6024],\n",
       "        [ 2.7805,  0.5125, -4.2648,  1.6131,  0.0497, -0.4633,  2.6239],\n",
       "        [ 0.7606,  0.1618,  2.6368, -0.8003,  0.8390, -3.0347,  3.6927],\n",
       "        [-2.6435, -0.2488, -3.1875, -2.1668,  0.2999,  0.7026,  0.6561],\n",
       "        [ 0.0000, -0.0000,  0.0000, -0.0000,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000, -0.0000,  0.0000, -0.0000,  0.0000, -0.0000]],\n",
       "       grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc = nn.Embedding(100, 7, padding_idx=1)\n",
    "enc_dp = EmbeddingDropout(enc, 0.5)\n",
    "tst_input = torch.randint(0, 100, (8,))\n",
    "print(tst_input)\n",
    "enc_dp(tst_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model  \n",
    "  \n",
    "The main model is a regular LSTM with several layers, but using all those kinds of dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def to_detach(h):\n",
    "    \"Detaches 'h' from its history\"\n",
    "    return h.detach() if type(h) == torch.Tensor else tuple(to_detach(v) for v in h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class AWD_LSTM(nn.Module):\n",
    "    \"AWD-LSTM inspired by https://arxiv.org/abs/1708.02182\"\n",
    "    initrange = 0.1\n",
    "    \n",
    "    def __init__(self, vocab_sz, emb_sz, n_hid, n_layers, pad_token,\n",
    "                hidden_p = 0.2, input_p = 0.6, embed_p = 0.1, weight_p = 0.5):\n",
    "        super().__init__()\n",
    "        self.bs, self.emb_sz, self.n_hid, self.n_layers = 1, emb_sz, n_hid, n_layers\n",
    "        self.emb = nn.Embedding(vocab_sz, emb_sz, padding_idx = pad_token) # get embedded matrix\n",
    "        self.emb_dp = EmbeddingDropout(self.emb, embed_p) # apply dropout on it with p=embed_p\n",
    "        self.rnns = [nn.LSTM(emb_sz if l == 0 else n_hid, (n_hid if l!= n_layers-1 else emb_sz), 1,\n",
    "                            batch_first=True) for l in range(n_layers)]\n",
    "        self.rnns = nn.ModuleList([WeightDropout(rnn, weight_p) for rnn in self.rnns]) # apply weight dropout\n",
    "        self.emb.weight.data.uniform_(-self.initrange, self.initrange) # re-initialize emb weights\n",
    "        self.input_dp = RNNDropout(input_p)\n",
    "        self.hidden_dps = nn.ModuleList([RNNDropout(hidden_p) for l in range(n_layers)]) # apply RNN dropout(with seq_dim)\n",
    "\n",
    "    def forward(self, input):\n",
    "        bs, sl = input.size()\n",
    "        if bs!=self.bs:\n",
    "            self.bs = bs\n",
    "            self.reset()\n",
    "        raw_output = self.input_dp(self.emb_dp(input))\n",
    "        new_hidden, raw_outputs, outputs = [], [], []\n",
    "        for l, (rnn, hid_dp) in enumerate(zip(self.rnns, self.hidden_dps)):\n",
    "            raw_output, new_h = rnn(raw_output, self.hidden[l])\n",
    "            new_hidden.append(new_h)\n",
    "            raw_outputs.append(raw_output)\n",
    "            if l!= self.n_layers - 1: raw_output = hid_dp(raw_output)\n",
    "            outputs.append(raw_output)\n",
    "        self.hidden = to_detach(new_hidden)\n",
    "        return raw_outputs, outputs\n",
    "    \n",
    "    def _one_hidden(self, l):\n",
    "        \"Return one hidden state\"\n",
    "        nh = self.n_hid if l != self.n_layers - 1 else self.emb_sz\n",
    "        return next(self.parameters()).new(1, self.bs, nh).zero_()\n",
    "    \n",
    "    def reset(self):\n",
    "        \"Reset the hidden states\"\n",
    "        self.hidden = [(self._one_hidden(l), self._one_hidden(l)) for l in range(self.n_layers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On top of this, we will apply a LinearDecoder. It's often best to use the same matrix as the one for the embeddings in the weights of the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearDecoder(nn.Module):\n",
    "    def __init__(self, n_out, n_hid, output_p, tie_encoder=None, bias = True):\n",
    "        super().__init__()\n",
    "        self.output_dp = RNNDropout(output_p)\n",
    "        self.decoder = nn.Linear(n_hid, n_out, bias=bias)\n",
    "        if bias: self.decoder.bias.data.zero_()\n",
    "        if tie_encoder: self.decoder.weight = tie_encoder.weight\n",
    "        else: init.kaiming_uniform_(self.decoder.weight)\n",
    "            \n",
    "    def forward(self, input):\n",
    "        raw_outputs, outputs = input\n",
    "#         https://discuss.pytorch.org/t/contigious-vs-non-contigious-tensor/30107/2\n",
    "        output = self.output_dp(outputs[-1]).contiguous()\n",
    "        decoded = self.decoder(output.view(output.size(0)* output.size(1), output.size(2)))\n",
    "        return decoded, raw_outputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class SequentialRNN(nn.Sequential):\n",
    "    \"A sequentioal module that passes the reset call to its children\"\n",
    "    def reset(self):\n",
    "        for c in self.children():\n",
    "            if hasattr(c, 'reset'): c.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we stack them all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def get_language_model(vocab_sz, emb_sz, n_hid, n_layers, pad_token, output_p=0.4, hidden_p=0.2, input_p=0.6,\n",
    "                      embed_p=0.1, weight_p=0.5, tie_weights=True, bias=True):\n",
    "    rnn_enc = AWD_LSTM(vocab_sz, emb_sz, n_hid, n_layers, pad_token, hidden_p, input_p, embed_p, weight_p)\n",
    "    enc = rnn_enc.emb if tie_weights else None\n",
    "    return nn.Sequential(rnn_enc, LinearDecoder(vocab_sz, emb_sz, output_p, tie_encoder=enc, bias=bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok_pad = vocab.index(PAD); tok_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can test all this works without throwing a bug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tst_model = get_language_model(len(vocab), 300, 300, 2, tok_pad)\n",
    "tst_model = tst_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(data.train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = tst_model(x.cuda())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We return 3 things to help with regularization: the true output(probabilities for each word), but also the activations of the encoder, with or without dropouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 0.0100,  0.0233, -0.0088,  ..., -0.0174, -0.0062, -0.0278],\n",
       "         [ 0.0162,  0.0337, -0.0112,  ..., -0.0290, -0.0095, -0.0409],\n",
       "         [ 0.0156,  0.0343, -0.0133,  ..., -0.0392, -0.0104, -0.0501],\n",
       "         ...,\n",
       "         [ 0.0238,  0.0215,  0.0019,  ..., -0.0474, -0.0016, -0.0190],\n",
       "         [ 0.0289,  0.0169,  0.0051,  ..., -0.0477,  0.0047, -0.0221],\n",
       "         [ 0.0259,  0.0204,  0.0076,  ..., -0.0472,  0.0020, -0.0238]],\n",
       "        device='cuda:0', grad_fn=<AddmmBackward>),\n",
       " [tensor([[[ 1.2877e-02,  2.0811e-02,  1.8974e-02,  ..., -1.4467e-02,\n",
       "            -2.4622e-02,  3.2236e-03],\n",
       "           [ 2.7131e-02,  2.9973e-02,  1.1575e-03,  ..., -1.0153e-02,\n",
       "            -1.0433e-02, -1.5177e-02],\n",
       "           [ 3.9571e-02,  1.5895e-02, -2.3793e-02,  ..., -2.8722e-02,\n",
       "             1.7880e-02, -5.6637e-02],\n",
       "           ...,\n",
       "           [ 6.8354e-02,  5.3641e-03, -4.4023e-02,  ..., -1.6555e-02,\n",
       "            -2.2625e-02, -5.2338e-02],\n",
       "           [ 3.4716e-02,  2.1746e-02, -2.7637e-02,  ..., -1.9809e-02,\n",
       "            -3.6494e-02, -4.5148e-02],\n",
       "           [ 4.0489e-02,  1.7364e-02,  9.0276e-03,  ...,  2.1179e-05,\n",
       "            -4.3639e-02, -3.0233e-02]],\n",
       "  \n",
       "          [[ 1.0992e-02, -2.8864e-03, -2.8901e-03,  ..., -5.1896e-03,\n",
       "             6.3153e-03, -5.6691e-03],\n",
       "           [ 1.4285e-02,  1.5319e-02, -1.7073e-02,  ..., -1.7615e-02,\n",
       "            -4.6462e-03, -2.4180e-02],\n",
       "           [ 2.7062e-02,  5.5709e-02, -1.6384e-02,  ..., -9.5222e-03,\n",
       "             4.0812e-03, -2.2695e-02],\n",
       "           ...,\n",
       "           [ 2.2221e-02,  4.0055e-02, -2.0105e-02,  ...,  8.6328e-04,\n",
       "             4.6642e-03, -4.0174e-02],\n",
       "           [ 3.0320e-02,  4.2035e-02, -2.0539e-02,  ..., -4.8136e-03,\n",
       "             6.6745e-04, -3.8343e-02],\n",
       "           [ 3.7803e-02,  3.2273e-02, -1.7578e-02,  ..., -2.9050e-03,\n",
       "            -1.9547e-02, -6.4263e-02]],\n",
       "  \n",
       "          [[ 1.8689e-02,  3.7104e-02, -1.7415e-02,  ...,  9.6081e-03,\n",
       "            -1.5432e-03, -4.0259e-02],\n",
       "           [ 1.0120e-02,  4.8550e-02, -4.2697e-02,  ..., -1.0996e-02,\n",
       "            -5.0174e-03, -1.9787e-02],\n",
       "           [ 2.4359e-02,  4.6680e-02, -3.3921e-02,  ..., -1.1129e-02,\n",
       "            -3.3186e-03, -3.0127e-02],\n",
       "           ...,\n",
       "           [ 3.0449e-02,  4.5896e-02, -5.1446e-03,  ...,  3.0211e-03,\n",
       "             1.3519e-02, -1.2719e-02],\n",
       "           [ 7.4446e-02,  4.6171e-02, -3.1471e-02,  ..., -1.9630e-02,\n",
       "             2.2567e-02, -3.8157e-02],\n",
       "           [ 7.2544e-02,  3.8677e-02, -2.2731e-02,  ..., -1.4922e-02,\n",
       "            -1.1672e-02, -5.1820e-02]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-2.7437e-03,  2.7292e-02, -1.8033e-02,  ..., -3.0360e-02,\n",
       "            -1.3714e-02, -8.9882e-03],\n",
       "           [ 2.7451e-02,  4.0466e-02, -3.2708e-02,  ..., -1.9233e-02,\n",
       "             2.4928e-02, -3.4664e-02],\n",
       "           [ 1.5405e-02,  1.6177e-02, -4.0187e-02,  ..., -3.2116e-02,\n",
       "             2.0315e-02, -4.2239e-02],\n",
       "           ...,\n",
       "           [ 5.3715e-02,  6.5531e-03, -3.5501e-02,  ..., -8.9034e-03,\n",
       "            -1.2479e-02, -4.0879e-02],\n",
       "           [ 7.0256e-02,  1.7252e-02, -1.8479e-02,  ...,  1.2338e-03,\n",
       "            -1.4083e-03, -4.7979e-02],\n",
       "           [ 5.7143e-02,  3.8243e-02, -2.8403e-02,  ..., -1.6246e-02,\n",
       "            -2.2163e-02, -2.6675e-02]],\n",
       "  \n",
       "          [[ 1.9134e-02,  4.5789e-03, -1.4312e-02,  ..., -3.6409e-03,\n",
       "            -2.1064e-02, -1.6375e-02],\n",
       "           [ 2.7050e-02,  2.4177e-02, -1.5592e-02,  ..., -1.1412e-02,\n",
       "            -1.4218e-02, -2.4277e-02],\n",
       "           [ 1.6756e-02,  2.8093e-02, -1.8395e-02,  ..., -1.5305e-02,\n",
       "            -1.4827e-02, -2.7709e-02],\n",
       "           ...,\n",
       "           [ 3.8414e-02,  4.4757e-02, -2.3684e-02,  ..., -5.8636e-03,\n",
       "             1.2862e-02, -5.0002e-02],\n",
       "           [ 7.4442e-02,  3.6803e-02, -1.5126e-02,  ...,  1.1296e-02,\n",
       "            -1.7835e-03, -3.9030e-02],\n",
       "           [ 5.6852e-02,  4.8816e-02, -3.9953e-02,  ...,  1.8538e-02,\n",
       "            -1.0302e-02, -5.1677e-02]],\n",
       "  \n",
       "          [[ 2.1832e-02,  2.5022e-02, -3.1015e-02,  ...,  1.9878e-03,\n",
       "            -2.8087e-02, -3.4773e-02],\n",
       "           [ 3.3512e-02,  4.2206e-02,  1.4070e-02,  ..., -1.6638e-02,\n",
       "            -9.4759e-03, -5.7891e-03],\n",
       "           [ 4.8482e-02,  3.8955e-02, -4.1534e-03,  ..., -9.3078e-03,\n",
       "            -2.7180e-02, -2.5817e-02],\n",
       "           ...,\n",
       "           [ 2.3969e-02,  2.5708e-02, -1.8220e-02,  ..., -3.0965e-03,\n",
       "            -2.0636e-02, -5.1574e-02],\n",
       "           [ 3.0623e-02,  2.4361e-02, -2.0491e-02,  ...,  2.3755e-03,\n",
       "             7.1386e-03, -3.5141e-02],\n",
       "           [ 2.9849e-02,  2.1600e-02, -2.1339e-02,  ..., -4.6254e-03,\n",
       "             4.2264e-03, -3.9367e-02]]], device='cuda:0',\n",
       "         grad_fn=<CudnnRnnBackward>),\n",
       "  tensor([[[-0.0098,  0.0032,  0.0125,  ..., -0.0067,  0.0095, -0.0038],\n",
       "           [-0.0174,  0.0029,  0.0211,  ..., -0.0064,  0.0176, -0.0032],\n",
       "           [-0.0229, -0.0012,  0.0298,  ..., -0.0070,  0.0246, -0.0027],\n",
       "           ...,\n",
       "           [-0.0358,  0.0101,  0.0344,  ...,  0.0051,  0.0372, -0.0004],\n",
       "           [-0.0358,  0.0086,  0.0320,  ...,  0.0074,  0.0325,  0.0040],\n",
       "           [-0.0385,  0.0072,  0.0297,  ...,  0.0073,  0.0337,  0.0104]],\n",
       "  \n",
       "          [[-0.0054, -0.0003,  0.0128,  ..., -0.0069,  0.0151,  0.0034],\n",
       "           [-0.0081, -0.0016,  0.0187,  ..., -0.0114,  0.0257,  0.0009],\n",
       "           [-0.0175, -0.0022,  0.0232,  ..., -0.0059,  0.0331, -0.0008],\n",
       "           ...,\n",
       "           [-0.0364,  0.0076,  0.0349,  ...,  0.0022,  0.0324,  0.0060],\n",
       "           [-0.0349,  0.0073,  0.0340,  ...,  0.0016,  0.0335,  0.0064],\n",
       "           [-0.0329,  0.0068,  0.0344,  ...,  0.0029,  0.0316,  0.0083]],\n",
       "  \n",
       "          [[-0.0065,  0.0044,  0.0097,  ..., -0.0082,  0.0142,  0.0014],\n",
       "           [-0.0132,  0.0071,  0.0169,  ..., -0.0120,  0.0214, -0.0004],\n",
       "           [-0.0204,  0.0060,  0.0209,  ..., -0.0101,  0.0270,  0.0010],\n",
       "           ...,\n",
       "           [-0.0330,  0.0150,  0.0236,  ..., -0.0068,  0.0423,  0.0163],\n",
       "           [-0.0359,  0.0144,  0.0242,  ..., -0.0064,  0.0430,  0.0175],\n",
       "           [-0.0391,  0.0114,  0.0233,  ..., -0.0042,  0.0431,  0.0154]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-0.0113,  0.0082,  0.0197,  ..., -0.0102,  0.0134, -0.0025],\n",
       "           [-0.0185,  0.0138,  0.0296,  ..., -0.0134,  0.0237, -0.0043],\n",
       "           [-0.0243,  0.0124,  0.0351,  ..., -0.0060,  0.0219, -0.0062],\n",
       "           ...,\n",
       "           [-0.0336,  0.0036,  0.0410,  ..., -0.0029,  0.0320,  0.0110],\n",
       "           [-0.0338,  0.0013,  0.0424,  ..., -0.0028,  0.0412,  0.0085],\n",
       "           [-0.0348,  0.0020,  0.0431,  ...,  0.0023,  0.0389,  0.0047]],\n",
       "  \n",
       "          [[-0.0054,  0.0014,  0.0172,  ..., -0.0080,  0.0096, -0.0013],\n",
       "           [-0.0139,  0.0036,  0.0259,  ..., -0.0091,  0.0177, -0.0010],\n",
       "           [-0.0230,  0.0024,  0.0294,  ..., -0.0100,  0.0253,  0.0028],\n",
       "           ...,\n",
       "           [-0.0341,  0.0148,  0.0300,  ..., -0.0054,  0.0254,  0.0039],\n",
       "           [-0.0333,  0.0140,  0.0347,  ..., -0.0079,  0.0278,  0.0054],\n",
       "           [-0.0338,  0.0171,  0.0327,  ..., -0.0047,  0.0288,  0.0043]],\n",
       "  \n",
       "          [[-0.0053, -0.0015,  0.0148,  ..., -0.0123,  0.0124, -0.0003],\n",
       "           [-0.0141, -0.0034,  0.0225,  ..., -0.0134,  0.0197,  0.0026],\n",
       "           [-0.0232, -0.0012,  0.0262,  ..., -0.0131,  0.0241,  0.0033],\n",
       "           ...,\n",
       "           [-0.0386,  0.0066,  0.0395,  ..., -0.0039,  0.0247,  0.0111],\n",
       "           [-0.0426,  0.0067,  0.0377,  ..., -0.0056,  0.0232,  0.0128],\n",
       "           [-0.0397,  0.0118,  0.0353,  ..., -0.0033,  0.0245,  0.0169]]],\n",
       "         device='cuda:0', grad_fn=<CudnnRnnBackward>)],\n",
       " [tensor([[[ 1.6096e-02,  2.6014e-02,  2.3717e-02,  ..., -1.8084e-02,\n",
       "            -3.0777e-02,  4.0294e-03],\n",
       "           [ 3.3914e-02,  3.7467e-02,  1.4469e-03,  ..., -1.2691e-02,\n",
       "            -1.3041e-02, -1.8971e-02],\n",
       "           [ 4.9464e-02,  1.9869e-02, -2.9741e-02,  ..., -3.5902e-02,\n",
       "             2.2350e-02, -7.0796e-02],\n",
       "           ...,\n",
       "           [ 8.5442e-02,  6.7051e-03, -5.5029e-02,  ..., -2.0694e-02,\n",
       "            -2.8281e-02, -6.5423e-02],\n",
       "           [ 4.3395e-02,  2.7182e-02, -3.4547e-02,  ..., -2.4761e-02,\n",
       "            -4.5617e-02, -5.6435e-02],\n",
       "           [ 5.0611e-02,  2.1705e-02,  1.1285e-02,  ...,  2.6474e-05,\n",
       "            -5.4548e-02, -3.7791e-02]],\n",
       "  \n",
       "          [[ 1.3740e-02, -3.6080e-03, -3.6126e-03,  ..., -6.4869e-03,\n",
       "             7.8941e-03, -7.0864e-03],\n",
       "           [ 1.7856e-02,  1.9149e-02, -2.1341e-02,  ..., -2.2019e-02,\n",
       "            -5.8077e-03, -3.0225e-02],\n",
       "           [ 3.3827e-02,  6.9636e-02, -2.0479e-02,  ..., -1.1903e-02,\n",
       "             5.1015e-03, -2.8369e-02],\n",
       "           ...,\n",
       "           [ 2.7776e-02,  5.0069e-02, -2.5132e-02,  ...,  1.0791e-03,\n",
       "             5.8302e-03, -5.0218e-02],\n",
       "           [ 3.7900e-02,  5.2544e-02, -2.5674e-02,  ..., -6.0170e-03,\n",
       "             8.3431e-04, -4.7928e-02],\n",
       "           [ 4.7254e-02,  4.0341e-02, -2.1973e-02,  ..., -3.6312e-03,\n",
       "            -2.4434e-02, -8.0329e-02]],\n",
       "  \n",
       "          [[ 0.0000e+00,  4.6379e-02, -2.1768e-02,  ...,  1.2010e-02,\n",
       "            -1.9290e-03, -5.0324e-02],\n",
       "           [ 0.0000e+00,  6.0687e-02, -5.3371e-02,  ..., -1.3745e-02,\n",
       "            -6.2718e-03, -2.4734e-02],\n",
       "           [ 0.0000e+00,  5.8350e-02, -4.2401e-02,  ..., -1.3912e-02,\n",
       "            -4.1482e-03, -3.7659e-02],\n",
       "           ...,\n",
       "           [ 0.0000e+00,  5.7369e-02, -6.4308e-03,  ...,  3.7764e-03,\n",
       "             1.6899e-02, -1.5898e-02],\n",
       "           [ 0.0000e+00,  5.7714e-02, -3.9339e-02,  ..., -2.4537e-02,\n",
       "             2.8209e-02, -4.7696e-02],\n",
       "           [ 0.0000e+00,  4.8346e-02, -2.8413e-02,  ..., -1.8652e-02,\n",
       "            -1.4590e-02, -6.4775e-02]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-3.4296e-03,  3.4115e-02, -2.2541e-02,  ..., -3.7950e-02,\n",
       "            -1.7142e-02, -0.0000e+00],\n",
       "           [ 3.4313e-02,  5.0582e-02, -4.0884e-02,  ..., -2.4041e-02,\n",
       "             3.1160e-02, -0.0000e+00],\n",
       "           [ 1.9256e-02,  2.0222e-02, -5.0234e-02,  ..., -4.0146e-02,\n",
       "             2.5393e-02, -0.0000e+00],\n",
       "           ...,\n",
       "           [ 6.7143e-02,  8.1914e-03, -4.4376e-02,  ..., -1.1129e-02,\n",
       "            -1.5599e-02, -0.0000e+00],\n",
       "           [ 8.7821e-02,  2.1565e-02, -2.3099e-02,  ...,  1.5422e-03,\n",
       "            -1.7604e-03, -0.0000e+00],\n",
       "           [ 7.1428e-02,  4.7804e-02, -3.5504e-02,  ..., -2.0307e-02,\n",
       "            -2.7704e-02, -0.0000e+00]],\n",
       "  \n",
       "          [[ 2.3918e-02,  5.7236e-03, -0.0000e+00,  ..., -4.5512e-03,\n",
       "            -2.6329e-02, -2.0469e-02],\n",
       "           [ 3.3812e-02,  3.0222e-02, -0.0000e+00,  ..., -1.4265e-02,\n",
       "            -1.7772e-02, -3.0346e-02],\n",
       "           [ 2.0945e-02,  3.5117e-02, -0.0000e+00,  ..., -1.9132e-02,\n",
       "            -1.8534e-02, -3.4636e-02],\n",
       "           ...,\n",
       "           [ 4.8018e-02,  5.5946e-02, -0.0000e+00,  ..., -7.3295e-03,\n",
       "             1.6078e-02, -6.2503e-02],\n",
       "           [ 9.3053e-02,  4.6003e-02, -0.0000e+00,  ...,  1.4119e-02,\n",
       "            -2.2294e-03, -4.8787e-02],\n",
       "           [ 7.1065e-02,  6.1020e-02, -0.0000e+00,  ...,  2.3172e-02,\n",
       "            -1.2877e-02, -6.4596e-02]],\n",
       "  \n",
       "          [[ 2.7290e-02,  3.1278e-02, -3.8769e-02,  ...,  0.0000e+00,\n",
       "            -0.0000e+00, -4.3466e-02],\n",
       "           [ 4.1890e-02,  5.2758e-02,  1.7588e-02,  ..., -0.0000e+00,\n",
       "            -0.0000e+00, -7.2363e-03],\n",
       "           [ 6.0603e-02,  4.8694e-02, -5.1917e-03,  ..., -0.0000e+00,\n",
       "            -0.0000e+00, -3.2272e-02],\n",
       "           ...,\n",
       "           [ 2.9961e-02,  3.2135e-02, -2.2775e-02,  ..., -0.0000e+00,\n",
       "            -0.0000e+00, -6.4468e-02],\n",
       "           [ 3.8279e-02,  3.0451e-02, -2.5614e-02,  ...,  0.0000e+00,\n",
       "             0.0000e+00, -4.3927e-02],\n",
       "           [ 3.7311e-02,  2.7000e-02, -2.6673e-02,  ..., -0.0000e+00,\n",
       "             0.0000e+00, -4.9209e-02]]], device='cuda:0', grad_fn=<MulBackward0>),\n",
       "  tensor([[[-0.0098,  0.0032,  0.0125,  ..., -0.0067,  0.0095, -0.0038],\n",
       "           [-0.0174,  0.0029,  0.0211,  ..., -0.0064,  0.0176, -0.0032],\n",
       "           [-0.0229, -0.0012,  0.0298,  ..., -0.0070,  0.0246, -0.0027],\n",
       "           ...,\n",
       "           [-0.0358,  0.0101,  0.0344,  ...,  0.0051,  0.0372, -0.0004],\n",
       "           [-0.0358,  0.0086,  0.0320,  ...,  0.0074,  0.0325,  0.0040],\n",
       "           [-0.0385,  0.0072,  0.0297,  ...,  0.0073,  0.0337,  0.0104]],\n",
       "  \n",
       "          [[-0.0054, -0.0003,  0.0128,  ..., -0.0069,  0.0151,  0.0034],\n",
       "           [-0.0081, -0.0016,  0.0187,  ..., -0.0114,  0.0257,  0.0009],\n",
       "           [-0.0175, -0.0022,  0.0232,  ..., -0.0059,  0.0331, -0.0008],\n",
       "           ...,\n",
       "           [-0.0364,  0.0076,  0.0349,  ...,  0.0022,  0.0324,  0.0060],\n",
       "           [-0.0349,  0.0073,  0.0340,  ...,  0.0016,  0.0335,  0.0064],\n",
       "           [-0.0329,  0.0068,  0.0344,  ...,  0.0029,  0.0316,  0.0083]],\n",
       "  \n",
       "          [[-0.0065,  0.0044,  0.0097,  ..., -0.0082,  0.0142,  0.0014],\n",
       "           [-0.0132,  0.0071,  0.0169,  ..., -0.0120,  0.0214, -0.0004],\n",
       "           [-0.0204,  0.0060,  0.0209,  ..., -0.0101,  0.0270,  0.0010],\n",
       "           ...,\n",
       "           [-0.0330,  0.0150,  0.0236,  ..., -0.0068,  0.0423,  0.0163],\n",
       "           [-0.0359,  0.0144,  0.0242,  ..., -0.0064,  0.0430,  0.0175],\n",
       "           [-0.0391,  0.0114,  0.0233,  ..., -0.0042,  0.0431,  0.0154]],\n",
       "  \n",
       "          ...,\n",
       "  \n",
       "          [[-0.0113,  0.0082,  0.0197,  ..., -0.0102,  0.0134, -0.0025],\n",
       "           [-0.0185,  0.0138,  0.0296,  ..., -0.0134,  0.0237, -0.0043],\n",
       "           [-0.0243,  0.0124,  0.0351,  ..., -0.0060,  0.0219, -0.0062],\n",
       "           ...,\n",
       "           [-0.0336,  0.0036,  0.0410,  ..., -0.0029,  0.0320,  0.0110],\n",
       "           [-0.0338,  0.0013,  0.0424,  ..., -0.0028,  0.0412,  0.0085],\n",
       "           [-0.0348,  0.0020,  0.0431,  ...,  0.0023,  0.0389,  0.0047]],\n",
       "  \n",
       "          [[-0.0054,  0.0014,  0.0172,  ..., -0.0080,  0.0096, -0.0013],\n",
       "           [-0.0139,  0.0036,  0.0259,  ..., -0.0091,  0.0177, -0.0010],\n",
       "           [-0.0230,  0.0024,  0.0294,  ..., -0.0100,  0.0253,  0.0028],\n",
       "           ...,\n",
       "           [-0.0341,  0.0148,  0.0300,  ..., -0.0054,  0.0254,  0.0039],\n",
       "           [-0.0333,  0.0140,  0.0347,  ..., -0.0079,  0.0278,  0.0054],\n",
       "           [-0.0338,  0.0171,  0.0327,  ..., -0.0047,  0.0288,  0.0043]],\n",
       "  \n",
       "          [[-0.0053, -0.0015,  0.0148,  ..., -0.0123,  0.0124, -0.0003],\n",
       "           [-0.0141, -0.0034,  0.0225,  ..., -0.0134,  0.0197,  0.0026],\n",
       "           [-0.0232, -0.0012,  0.0262,  ..., -0.0131,  0.0241,  0.0033],\n",
       "           ...,\n",
       "           [-0.0386,  0.0066,  0.0395,  ..., -0.0039,  0.0247,  0.0111],\n",
       "           [-0.0426,  0.0067,  0.0377,  ..., -0.0056,  0.0232,  0.0128],\n",
       "           [-0.0397,  0.0118,  0.0353,  ..., -0.0033,  0.0245,  0.0169]]],\n",
       "         device='cuda:0', grad_fn=<CudnnRnnBackward>)])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded, raw_outputs, outputs = z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decoded tensor is flattended to `bs*seq_len` by `len(vocab)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4480, 60003])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`raw_outputs` and `outputs` each contain the results of intermediary layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw_outputs), len(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([torch.Size([64, 70, 300]), torch.Size([64, 70, 300])],\n",
       " [torch.Size([64, 70, 300]), torch.Size([64, 70, 300])])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[o.size() for o in raw_outputs], [o.size() for o in outputs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calbacks to train the model  \n",
    "  \n",
    "We need to add a few tweaks to train a language model: first we will clip the gradients. This is a classic technique that will allow us to use a high leraning rate by putting a maximum value on the norm of the gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class GradientClipping(Callback):\n",
    "    def __init__(self, clip=None): self.clip= clip\n",
    "    def after_backward(self):\n",
    "        if self.clip: nn.utils.clip_grad_norm_(self.run.model.parameters(), self.clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we add an `RnnTrainer` that will do the following 4 things:\n",
    "- Change the output to make it contain only the `decoded` tensor(for the loss function) and store the `raw_outputs` and `outputs`.\n",
    "- Apply Activation Regularization(AR): we add to the loss a L2 penalty on the last activations of the AWD-LSTM(with dropout applied).\n",
    "- apply Temporal Activation Regularisation(TAR): we add to the loss a L2 penalty on the difference between 2 consecutive(in terms of words) terms of raw_outputs.\n",
    "- Trigger the shuffle of LM_Dataset at the beginning of each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class RNNTrainer(Callback):\n",
    "    def __init__(self, α, β): self.α, self.β = α, β\n",
    "    \n",
    "    def after_pred(self):\n",
    "        # Save the extra outputs for later and only returns the true output\n",
    "        self.raw_out, self.out = self.pred[1], self.pred[2]\n",
    "        self.run.pred = self.pred[0]\n",
    "        \n",
    "    def after_loss(self):\n",
    "        # Applying AR and TAR\n",
    "        if self.α != 0 : self.run.loss += self.α * self.out[-1].float().pow(2).mean()\n",
    "        if self.β != 0: \n",
    "            h = self.raw_out[-1]\n",
    "            if h.size(1)>1: self.run.loss += self.β * (h[:,1:] - h[:,:-1]).float().pow(2).mean()\n",
    "     \n",
    "    def begin_batch(self):\n",
    "        # SHuffle the texts at the beginning of the epoch\n",
    "        if hasattr(self.dl.dataset, 'batchify'): self.dl.dataset.batchify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we write a flattened version of the cross entropy loss and the accuracy metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def cross_entropy_flat(input, target):\n",
    "    bs, sl = target.size()\n",
    "    return F.cross_entropy(input.view(bs*sl, -1), target.view(bs * sl))\n",
    "\n",
    "def accuracy_flat(input, target):\n",
    "    bs , sl = target.size()\n",
    "    return accuracy(input.view(bs*sl, -1), target.view(bs * sl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_sz, nh, nl = 300, 300, 2\n",
    "model = get_language_model(len(vocab), emb_sz, nh, nl, tok_pad, input_p=0.6, output_p=0.4, weight_p=0.5,\n",
    "                          embed_p=0.1, hidden_p=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbfs = [partial(AvgStatsCallback, accuracy_flat),\n",
    "       CudaCallback, Recorder,\n",
    "       partial(GradientClipping, clip=0.1),\n",
    "       partial(RNNTrainer, α=2., β=1.),\n",
    "       ProgressCallback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = Learner(model, data, cross_entropy_flat, lr=5e-3, cb_funcs=cbfs, opt_func=adam_opt())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learn.fit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 12a_awd_lstm.ipynb to exp/nb_12a.py\n"
     ]
    }
   ],
   "source": [
    "!python notebook2script.py 12a_awd_lstm.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
